<p align="center">
  <img width="200" height="200" src="https://github.com/user-attachments/assets/9970c90e-bcf2-4764-94b8-46aa7de231f4" alt="å¤§æ¨¡åž‹" />
</p>

<h2 align="center">
  Benchmarking Essay Analysis Ability of Large Language Models
</h2>

<p align="center">
  <strong>
    <img width="25" height="25" alt="æ•°æ®åº“" src="https://github.com/user-attachments/assets/2626d4d3-3eae-49fe-8680-098fe4931827" />
  </strong>
  <a href="#"> Data </a>
</p>

## ðŸ‘¾ Instroduction
Large language models (LLMs) possess strong semantic understanding capabilities, giving them a natural advantage in automated essay scoring (AES). So far, they have demonstrated remarkable performance in this field. However, existing LLM-based AES methods for chinese narrative essays exhibit biases in terms of evaluation trait comprehensiveness and scoring reliability. This problem directly affects educators' acceptance of LLM-based AES technologies. To bridge this gap, we introduce ElionBench, a comprehensive benchmark designed to evaluate the comprehensive performance of LLMs on AES. ElionBench assesses LLMs for chinese narrative essays based on three aspects: 1) Foundational Capabilities: evaluate the performance of LLMs in total score and each trait; 2) Generalization Capability: measure the adaptability and robustness of LLMs in cross-prompt scenarios; 3) Bias Calibration: assess the bias of LLMs in AES tasks and perform bias calibration mechanisms. ElionBench aims to provide a comprehensive evaluation of LLM capabilities in AES for Chinese narrative essays. This can promote the advancement of LLMs in the field of essay data analysis. The code is made publicly available at the URL.


ðŸ˜º Update soon...
